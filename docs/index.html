

<!DOCTYPE html>
<html>
<head>
	<title>Replacing Labeled Real-Image Datasets With Auto-Generated Contours</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
    <!--<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">-->
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
</style>

<style>
a.link {
    text-decoration: underline
}
</style>


<h1 align="center" style="font-size: 30pt;"><b>Replacing Labeled Real-Image Datasets<br>with Auto-Generated Contours</b></h1><br/>

<center>
    <font color="#c7254e"><b>IEEE/CVF International Conference on Computer Vision and Pattern Recognition<br>(CVPR 2022)</b></font><br><br>
    <a href="http://hirokatsukataoka.net/" class="">Hirokatsu Kataoka</a><sup>1</sup> &emsp;
    <a href="" class="">Ryo Hayamizu</a><sup>1</sup> &emsp;
    <a href="https://scholar.google.nl/citations?user=2nmJ6qQAAAAJ&hl=en" class="">Ryosuke Yamada</a><sup>1</sup> &emsp;
    <a href="https://twitter.com/kodai_nakashima" class="">Kodai Nakashima</a><sup>1</sup> &emsp;
    <a href="https://masora1030.github.io/soraemonpockt/" class="">Sora Takashima</a><sup>1,2</sup> &emsp;<br>
    <a href="https://zhxyxxx.github.io/" class="">Xinyu Zhang</a><sup>1,2</sup> &emsp;
    <a href="https://www.linkedin.com/in/edgar-josafat-martinez-noriega-abb76378/?originalSubdomain=jp" class="">Edgar Josafat Martinez-Noriega</a><sup>1,2</sup> &emsp;    
    <a href="https://mmai.tech/" class="">Nakamasa Inoue</a><sup>1,2</sup> &emsp; 
    <a href="https://www.rio.gsic.titech.ac.jp/en/index.html" class="">Rio Yokota</a><sup>1,2</sup> &emsp;
<br>
    1: AIST &emsp; 2: TITech<br><br>
    
    <section class="delta">
        <div class="container">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kataoka_Replacing_Labeled_Real-Image_Datasets_With_Auto-Generated_Contours_CVPR_2022_paper.html"><button class="btn btn-gray">Paper</button></a>
            <a href="https://github.com/masora1030/CVPR2022-Pretrained-ViT-PyTorch"><button class="btn btn-gray">Code</button></a>
            <a href="#dataset"><button class="btn btn-gray">Dataset</button></a>
            <a href="http://hirokatsukataoka.net/pdf/cvpr22_kataoka_oral.pdf"><button class="btn btn-gray">Oral</button></a>
            <a href="http://hirokatsukataoka.net/pdf/cvpr22_kataoka_poster.pdf"><button class="btn btn-gray">Poster</button></a>
            <a href="http://hirokatsukataoka.net/pdf/cvpr22_kataoka_fdsl_supplementary.pdf"><button class="btn btn-gray">Supp</button></a> <br>
            <a href="https://hirokatsukataoka16.github.io/Pretraining-without-Natural-Images/"><button class="btn btn-gray">Related Work 1 (IJCV22)<br>FDSL Proposal</button></a>
            <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/"><button class="btn btn-gray">Related Work 2 (AAAI22)<br>ViT Pre-training</button></a>
            <a href="https://ryosuke-yamada.github.io/PointCloud-FractalDataBase/"><button class="btn btn-gray">Related Work 3 (CVPR22)<br>Point Cloud Pre-training</button></a>
        </div>
    </section>
    <br><br>
    <iframe width="800" height="450" src="https://www.youtube.com/embed/d-NagM4nGIQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <!--<img src="./img/teaser.png" style="width: 100%;"/>-->
</center>

<br>
<h2>Abstract</h2>
<p>
<font color="#c7254e" font size="5"><b>In the present work, we show that the performance of formula-driven supervised learning (FDSL) can match or even exceed that of ImageNet-21k without the use of real images, human-, and self-supervision during the pre-training of Vision Transformers (ViTs).</b></font> For example, ViT-Base pre-trained on ImageNet-21k shows 81.8% top-1 accuracy when fine-tuned on ImageNet-1k and FDSL shows 82.7% top-1 accuracy when pre-trained under the same conditions (number of images, hyperparameters, and number of epochs; see figure below). Images generated by formulas avoid the privacy/copyright issues, labeling cost and errors, and biases that real images suffer from, and thus have tremendous potential for pre-training general models. To understand the performance of the synthetic images, we tested two hypotheses, namely (i) object contours are what matter in FDSL datasets and (ii) increased number of parameters to create labels affects performance improvement in FDSL pre-training. To test the former hypothesis, we constructed a dataset that consisted of simple object contour combinations. We found that this dataset can match the performance of fractals. For the latter hypothesis, we found that increasing the difficulty of the pre-training task generally leads to better fine-tuning accuracy.
</p>

<br><br><br>
<h2>Two hypotheses</h2>

To enhance the performance of FDSL, we test the following hypotheses 1 & 2:<br><br><br>

<font size="4">Hypothesis 1: Object Contours are what matter in FDSL datasets</font>
<ul>
  <li>In our preliminary study we found that attention was focused on the outer contours of the fractals</li>
  <li>We created a new dataset that consists only of contours â€“ Radial Contour DataBase (RCDB)</li>
  <li>Despite the lack of any texture, RCDB performed close to FractalDB and outperformed ImageNet-21k</li>
</ul>
<center>
        <img src="./img/hypothesis1.png" style="width: 60%;"/>
        <img src="./img/h1_exp.png" style="width: 39%;"/>
</center><br><br><br>

<font size="4">Hypothesis 2: Increased number of parameters in FDSL pre-training</font>
<ul>
  <li>We tested various synthetic datasets with varying complexity of images</li>
  <li>For RCDB, we changed the number of polygons, radius, line width, resizing factor, and Perlin noise</li>
  <li>Complex images increases the difficulty of the pre-training task and leads to better downstream performance</li>
</ul>
<center>
        <img src="./img/hypothesis2.png" style="width: 54%;"/> 
        <img src="./img/h2_exp.png" style="width: 45%;"/>
</center>

<br><br><br>
<h2>Samples images from our FDSL datasets</h2>

<center>
        <img src="./img/sample_images.png" style="width: 100%;"/> 
</center>

<br><br><br>
<h2>Comparison: ImageNet-1k, MS COCO, Other datasets</h2>

We have found that vision transformers (ViT) can be successfully pre-trained without real images, human- and self- supervision, and can exceed the accuracy of ImageNet-21k pre- training when fine-tuned on ImageNet-1k. We constructed a new dataset Radial Contour DataBase (RCDB) based on the as- sumption that contours are what matter for the pre-training of ViT. RCDB also exceeded the performance of ImageNet-21k pre- training, while consisting only of contours.
<center>
        <img src="./img/comparison_fdsl_sl.png" style="width: 100%;"/>
</center>
<br>

(Left) Comparison of ImageNet-1k fine-tuning. Accuracies obtained with ViT-Ti/B architectures are listed. 21k/50k indi- cates the number of classes in the pre-training phase. Best and second-best values for a given dataset are in underlined bold and bold, respectively. (Right) Comparison of object detection and instance segmen- tation. Several pre-trained models were validated on COCO dataset. Best values at each learning type are in bold.
<center>
        <img src="./img/comparison_imagenet1k_mscoco.png" style="width: 100%;"/>
</center>
<br>

Comparison of pre-training for SL/SSL methods. For SSL, (D) indicates DINO. Best values at each learning type are in bold.
<center>
        <img src="./img/comparison_ft.png" style="width: 100%;"/>
</center>

<br><br><br>
<h2>Failure modes: FractalDB and RCDB</h2>

We investigate the minimum number of points used in the rendering of fractals in FractalDB. The following figures shows the results and image examples in the point-rendered FractalDB. According to the performance rates in the figures, the pre-trained models acquire a good representation when the number of fractal points is 50k or higher, at which point the fractal images start to form a contour.
<center>
        <img src="./img/failuremodes_fractaldb.png" style="width: 80%;"/>
</center>
<br><br><br>

We verify RCDB images with and without broken object contours, as shown in the following figures. We deliberately draw 1k lines with the same color as the background. The lengths and positions of the lines are fully randomized. We adjust the thickness of the lines so that the object contours of RCDB are corrupted but the main frame does not disappear like in the figures. 
<center>
        <img src="./img/failuremodes_rcdb.png" style="width: 80%;"/>
</center>

<br>
<h2>Citation</h2>
@InProceedings{Kataoka_2022_CVPR,<br>
&emsp;author    = {Kataoka, Hirokatsu and Hayamizu, Ryo and Yamada, Ryosuke and Nakashima, Kodai and Takashima, Sora and Zhang, Xinyu and Martinez-Noriega, Edgar Josafat and Inoue, Nakamasa and Yokota, Rio},<br>
&emsp;title     = {Replacing Labeled Real-Image Datasets With Auto-Generated Contours},<br>
&emsp;booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
&emsp;month     = {June},<br>
&emsp;year      = {2022},<br>
&emsp;pages     = {21232-21241}<br>
}<br><br>

<a name="dataset"><h2>Dataset Download</h2></a>
<ul>
    <li>
        ExFractalDB-1k (1k categories x 1k instances; Total 1M images).
        <a href="https://drive.google.com/file/d/1KKqz0H7i_TXFMa2oJtcfry9bmAxyS_SS/view?usp=sharing">[Dataset (13GB); TBD]</a>
    </li>
    <li>
        RCDB-1k (1k categories x 1k instances; Total 1M images).
        <a href="https://drive.google.com/file/d/1F0aEogTScpABjJhNZJaCFT-J8mkdP86o/view?usp=sharing">[Dataset (13GB); TBD]</a>
    </li>
</ul>
<br><br>

<h2>Acknowledgement</h2></a>
<ul>
    <li> This work is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</li>
    <li> Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</li>
</ul>


<script type="text/javascript" src="./footer.js"></script>
</body></html>